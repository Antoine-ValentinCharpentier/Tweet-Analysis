{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification des données en utilisant le SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import sklearn as sk\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On prendre 3 variables pour le SVM car les donnés sont issus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp= pd.read_csv(\"../../data/export_data_labeled.csv\",header=None)\n",
    "df=tmp.head(20000)\n",
    "X1=df.drop(df.columns[-1], axis=1)\n",
    "X=X1.drop(df.columns[[3,4,5,6,7,8,9]], axis=1).values\n",
    "labels=df.iloc[:, -1].replace([1.0,2.0,4.0], 0)\n",
    "labels=labels.replace(3.0, 1)\n",
    "Y=labels.values\n",
    "print(labels.value_counts())\n",
    "print(X)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation des données labélisées en Apprentisage , Test et Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification avec SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Déterminaison de la meilleure combinaison d'hyperparamètres"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition d'une fonction pour afficher une matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Création de la figure\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Création de la heatmap\n",
    "    heatmap = ax.imshow(cm, cmap='Blues')\n",
    "\n",
    "    # Ajout des valeurs dans les cellules de la heatmap\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm[i])):\n",
    "            ax.text(j, i, cm[i][j], ha='center', va='center', color='black')\n",
    "\n",
    "    # Définition des étiquettes des axes\n",
    "    classes = ['Normal', 'Atypique']\n",
    "    ax.set_xticks(range(len(classes)))\n",
    "    ax.set_yticks(range(len(classes)))\n",
    "    ax.set_xticklabels(classes)\n",
    "    ax.set_yticklabels(classes)\n",
    "\n",
    "    # Ajout d'une barre de couleur\n",
    "    cbar = ax.figure.colorbar(heatmap, ax=ax)\n",
    "\n",
    "    # Ajout des titres\n",
    "    ax.set_xlabel('Prédictions')\n",
    "    ax.set_ylabel('Vraies étiquettes')\n",
    "    ax.set_title('Matrice de confusion')\n",
    "\n",
    "    # Affichage de la figure\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des hyperparamètres à essayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'C': [1e-2, 1e-1, 1, 1e1],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre cas où la classe négative représente environ 84% des échantillons, utiliser le rappel seul pourrait être trompeur. Le rappel mesure la capacité d'un modèle à identifier correctement les échantillons positifs parmi tous les échantillons positifs réels. \n",
    "\n",
    "En utilisant ``balanced_accuracy``, on donne une importance égale aux performances des deux classes. Cela permet de s'assurer que notre modèle ne se concentre pas uniquement sur la classe majoritaire (négatif), mais qu'il est également capable de prédire correctement la classe minoritaire (postive). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmc = SVC()\n",
    "grille = GridSearchCV(estimator=svmc, param_grid=parameters, scoring='balanced_accuracy', cv=2, verbose=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécuter la recherche de grille pour trouver la meilleure configuration de modèle en ajustant les modèles sur les données d'apprentissage et en évaluant leur performance à l'aide de la validation croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats = grille.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage du meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Le meilleur modèle :', resultats.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats.cv_results_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = resultats.best_estimator_\n",
    "y_true = y_test\n",
    "y_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On regarde si on aurait obtenu de meilleurs résultats/un autre meilleur kernel avec d'autres fonctions de scoring pour le GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_scoring_value(scoring):\n",
    "    print('>>> Scoring => ', scoring)\n",
    "    # determination du meilleur kernel pour la fonction de scoring\n",
    "    svmc = SVC()\n",
    "    grille = GridSearchCV(estimator=svmc, param_grid=parameters, scoring=scoring, cv=2)\n",
    "    resultats = grille.fit(X_train, y_train)\n",
    "    print('Le meilleur modèle :', resultats.best_params_)\n",
    "    \n",
    "    # confusion matrix & accuracy\n",
    "    svm = resultats.best_estimator_\n",
    "    y_true = y_test\n",
    "    y_pred = svm.predict(X_test)\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(accuracy_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorings = ['accuracy', 'balanced_accuracy', 'top_k_accuracy', 'average_precision', 'neg_brier_score', 'f1', 'neg_log_loss', 'precision', 'recall', 'jaccard', 'roc_auc']\n",
    "for s in scorings:\n",
    "    test_scoring_value(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recherche des meilleurs attributs au vu de leur impact sur l'erreur"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération des hyperparamètres optimaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    C_opti = resultats.best_params_['C']\n",
    "    kernel_opti = resultats.best_params_['kernel']\n",
    "    gamma_opti = resultats.best_params_['gamma']\n",
    "except:\n",
    "    C_opti = 10\n",
    "    kernel_opti = 'rbf'\n",
    "    gamma_opti = 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impact=[]\n",
    "for i in range(1,10):\n",
    "    print(\"####### NOMBRE DE COMPOSANTES PRINCIPALES \",i)\n",
    "    X1=df.drop(df.columns[-1], axis=1)\n",
    "    X=X1.iloc[:, :i].values\n",
    "    labels=df.iloc[:, -1].replace([1.0,2.0,4.0], 0)\n",
    "    labels=labels.replace(3.0, 1)\n",
    "    Y=labels.values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3,random_state=42)\n",
    "    \n",
    "    svm=SVC(C=C_opti,kernel=kernel_opti, gamma=gamma_opti)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred=svm.predict(X_test)\n",
    "    y_true = y_test\n",
    "    print(classification_report(y_true,y_pred))\n",
    "    impact.append({\"Nombre de CP\" :i,\"accuracy\":accuracy_score(y_true,y_pred)})\n",
    "\n",
    "    #display_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = [d['Nombre de CP'] for d in impact]\n",
    "y_values = [d['accuracy'] for d in impact]\n",
    "\n",
    "plt.plot(x_values, y_values, marker='o')\n",
    "plt.xlabel('Nombre de CP')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Evolution de la précision avec le nombre de CP')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=Y,edgecolors='k', cmap=plt.cm.coolwarm)\n",
    "legend = plt.legend(*scatter.legend_elements(), title='Label')\n",
    "# Afficher le graphique 2D\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title(\"SVM RBF All\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Tracer les points en 3D avec une couleur basée sur la dimension supplémentaire\n",
    "sc = ax.scatter(X[:, 0], X[:, 1],X[:, 2], c=Y, cmap=plt.cm.coolwarm,edgecolors='k')\n",
    "legend = ax.legend(*sc.legend_elements(), title='Label')\n",
    "ax.add_artist(legend)\n",
    "\n",
    "# Ajouter des labels aux axes\n",
    "ax.set_xlabel('Dim 1')\n",
    "ax.set_ylabel('Dim 2')\n",
    "ax.set_zlabel('Dim 3')\n",
    "\n",
    "# Afficher le graphique 3D\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
