{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification des données en utilisant le SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import sklearn as sk\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client[\"Tweet\"]\n",
    "user_collection = db[\"users_labeled_scaled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(user_collection.find({}))\n",
    "users = pd.DataFrame(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users.drop(columns=[\"_id\",\"friends_count\",\"followers_count\",\"tweet_frequency\"])\n",
    "print(users.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=users.label\n",
    "X=users.drop(columns=[\"label\"])\n",
    "attributs=[att for att in X.columns]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig = pd.DataFrame(\n",
    "    {\n",
    "        \"Dimension\" : [\"Dim\" + str(x + 1) for x in range(len(attributs))], \n",
    "        \"Variance expliquée\" : pca.explained_variance_,\n",
    "        \"% variance expliquée\" : np.round(pca.explained_variance_ratio_ * 100),\n",
    "        \"% cum. var. expliquée\" : np.round(np.cumsum(pca.explained_variance_ratio_) * 100)\n",
    "    }\n",
    ")\n",
    "eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig.plot.bar(x = \"Dimension\", y = \"% cum. var. expliquée\") # permet un diagramme en barres\n",
    "plt.axhline(y = 80, linewidth = .5, color = \"dimgray\", linestyle = \"--\")\n",
    "plt.axhline(y = 90, linewidth = .5, color = \"dimgray\", linestyle = \"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 8  # Remplacez par le nombre de composantes principales souhaitées\n",
    "X_pca = pca.transform(X)[:, :n_components]\n",
    "print(X_pca)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation des données labélisées en Apprentisage , Test et Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification avec SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Déterminaison de la meilleure combinaison d'hyperparamètres"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition d'une fonction pour afficher une matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Création de la figure\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Création de la heatmap\n",
    "    heatmap = ax.imshow(cm, cmap='Blues')\n",
    "\n",
    "    # Ajout des valeurs dans les cellules de la heatmap\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm[i])):\n",
    "            ax.text(j, i, cm[i][j], ha='center', va='center', color='black')\n",
    "\n",
    "    # Définition des étiquettes des axes\n",
    "    classes = ['Normal', 'Atypique']\n",
    "    ax.set_xticks(range(len(classes)))\n",
    "    ax.set_yticks(range(len(classes)))\n",
    "    ax.set_xticklabels(classes)\n",
    "    ax.set_yticklabels(classes)\n",
    "\n",
    "    # Ajout d'une barre de couleur\n",
    "    cbar = ax.figure.colorbar(heatmap, ax=ax)\n",
    "\n",
    "    # Ajout des titres\n",
    "    ax.set_xlabel('Prédictions')\n",
    "    ax.set_ylabel('Vraies étiquettes')\n",
    "    ax.set_title('Matrice de confusion')\n",
    "\n",
    "    # Affichage de la figure\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des hyperparamètres à essayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'C': [1e-2, 1e-1, 1, 1e1],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre cas où la classe négative représente environ 84% des échantillons, utiliser le rappel seul pourrait être trompeur. Le rappel mesure la capacité d'un modèle à identifier correctement les échantillons positifs parmi tous les échantillons positifs réels. \n",
    "\n",
    "En utilisant ``balanced_accuracy``, on donne une importance égale aux performances des deux classes. Cela permet de s'assurer que notre modèle ne se concentre pas uniquement sur la classe majoritaire (négatif), mais qu'il est également capable de prédire correctement la classe minoritaire (postive). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmc = SVC()\n",
    "grille = GridSearchCV(estimator=svmc, param_grid=parameters, scoring='balanced_accuracy', cv=2, verbose=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécuter la recherche de grille pour trouver la meilleure configuration de modèle en ajustant les modèles sur les données d'apprentissage et en évaluant leur performance à l'aide de la validation croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats = grille.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage du meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Le meilleur modèle :', resultats.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats.cv_results_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = resultats.best_estimator_\n",
    "y_true = y_test\n",
    "y_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On regarde si on aurait obtenu de meilleurs résultats/un autre meilleur kernel avec d'autres fonctions de scoring pour le GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_scoring_value(scoring):\n",
    "    print('>>> Scoring => ', scoring)\n",
    "    # determination du meilleur kernel pour la fonction de scoring\n",
    "    svmc = SVC()\n",
    "    grille = GridSearchCV(estimator=svmc, param_grid=parameters, scoring=scoring, cv=2)\n",
    "    resultats = grille.fit(X_train, y_train)\n",
    "    print('Le meilleur modèle :', resultats.best_params_)\n",
    "    \n",
    "    # confusion matrix & accuracy\n",
    "    svm = resultats.best_estimator_\n",
    "    y_true = y_test\n",
    "    y_pred = svm.predict(X_test)\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(accuracy_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorings = ['accuracy', 'balanced_accuracy', 'top_k_accuracy', 'average_precision', 'neg_brier_score', 'f1', 'neg_log_loss', 'precision', 'recall', 'jaccard', 'roc_auc']\n",
    "for s in scorings:\n",
    "    test_scoring_value(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recherche des meilleurs attributs au vu de leur impact sur l'erreur"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération des hyperparamètres optimaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    C_opti = resultats.best_params_['C']\n",
    "    kernel_opti = resultats.best_params_['kernel']\n",
    "    gamma_opti = resultats.best_params_['gamma']\n",
    "except:\n",
    "    C_opti = 10\n",
    "    kernel_opti = 'rbf'\n",
    "    gamma_opti = 'auto'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppression un par un des attributs jusqu'à avoir l'erreur minimale  ( INUTILE AVEC L'ACP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributs_removed = []\n",
    "err_fix=1-accuracy_score(y_true,y_pred)\n",
    "#err_fix=0.04566666666666663\n",
    "verif=True\n",
    "X_best=X_pca\n",
    "\n",
    "verif_count = 0\n",
    "\n",
    "while (verif):\n",
    "    print('verif n°', verif_count)\n",
    "    verif_count += 1\n",
    "    impact=[]\n",
    "    for att in range(X_best.shape[1]) :\n",
    "        print('Trying to remove', att)\n",
    "        \n",
    "        Y_clean=Y\n",
    "        X_clean=np.delete(X_best, att, axis=1)\n",
    "\n",
    "        X_tmp, X_test, y_tmp, y_test = train_test_split(X_clean, Y_clean, test_size=0.3)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.5)\n",
    "        \n",
    "        svm=SVC(C=C_opti,kernel=kernel_opti, gamma=gamma_opti)\n",
    "        svm.fit(X_train, y_train)\n",
    "        y_pred=svm.predict(X_test)\n",
    "\n",
    "        erreur=1-accuracy_score(y_test,y_pred)\n",
    "        impact.append({\"attribut\" :att,\"erreur\":erreur})\n",
    "\n",
    "    #On retire l'attribut qui contribue le plus à  l'erreur \n",
    "    impact.sort(key=lambda x: x['erreur'])\n",
    "    err_cal=impact[0]['erreur']\n",
    "    print(\"erreur de base  : \",err_fix,\"vs    erreur actuelle : \",err_cal)\n",
    "    if(err_cal>err_fix):\n",
    "        verif=False\n",
    "    else:\n",
    "        attribut_neg=impact[0]['attribut']\n",
    "        attributs_removed.append(attribut_neg)\n",
    "        X_best=np.delete(X_best, attribut_neg, axis=1)\n",
    "        print(impact[0])\n",
    "        err_fix=err_cal\n",
    "    if(X_pca.shape[1]==2):\n",
    "        verif=False\n",
    "print('END')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage des attributs qui doivent être gardés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage des attributs supprimés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributs_removed\n",
    "print(X_best)\n",
    "X_tmp, X_test, y_tmp, y_test = train_test_split(X_best, Y_clean, test_size=0.3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On regarde si la matrice de confusion a changée avec les attributs en moins. Si cela a augmenté ou non la précision ou a eu une influence sur le classification_report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train.drop(columns=attributs_removed)\n",
    "#X_test = X_test.drop(columns=attributs_removed)\n",
    "\n",
    "svm=SVC(C=C_opti,kernel=kernel_opti, gamma=gamma_opti)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred=svm.predict(X_test)\n",
    "y_true = y_test\n",
    "erreur=1-accuracy_score(y_test,y_pred)\n",
    "print(erreur)\n",
    "\n",
    "display_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
