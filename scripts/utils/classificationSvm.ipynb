{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification des données en utilisant le SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import sklearn as sk\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client[\"Tweet\"]\n",
    "user_collection = db[\"users_labeled_scaled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(user_collection.find({}))\n",
    "users = pd.DataFrame(users)\n",
    "#users.index = users.user_id\n",
    "\n",
    "#Normalement ce bloc va être supprimé une fois que le traitement aura été fait ailleurs\n",
    "users = users.drop(columns=[\"_id\",\"friends_count\",\"followers_count\",\"tweet_frequency\"])\n",
    "print(users.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation des données labélisées en Apprentisage , Test et Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standartisation sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=users.label\n",
    "X=users.drop(columns=[\"label\"])\n",
    "attributs=[att for att in X.columns]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification avec SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Déterminaison de la meilleure combinaison d'hyperparamètres"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un contexte de déséquilibre de classe, l'utilisation du rappel comme métrique de performance dans un modèle de classification binaire est justifiée. \n",
    "\n",
    "Le rappel mesure la capacité du modèle à trouver tous les échantillons positifs, ce qui est crucial lorsque les échantillons positifs sont rares ou représentent une classe minoritaire. \n",
    "\n",
    "En mettant l'accent sur la sensibilité à détecter les échantillons positifs, le rappel permet d'identifier les cas importants et de minimiser les faux négatifs, sans accorder une attention excessive aux faux positifs.\n",
    "\n",
    "\n",
    "Selon la documentation de scikit learn:\n",
    "> The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "\n",
    "Cette métrique est alors la mieux placé pour évaluer les performances de notre modèle car nous cherchons à classer les utilisateurs en deux classes (atypique ou non) et que la majorité des utilisateurs ne l'est pas (présence d'une majorité négative)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition d'une fonction pour afficher une matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Création de la figure\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Création de la heatmap\n",
    "    heatmap = ax.imshow(cm, cmap='Blues')\n",
    "\n",
    "    # Ajout des valeurs dans les cellules de la heatmap\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm[i])):\n",
    "            ax.text(j, i, cm[i][j], ha='center', va='center', color='black')\n",
    "\n",
    "    # Définition des étiquettes des axes\n",
    "    classes = ['Normal', 'Atypique']\n",
    "    ax.set_xticks(range(len(classes)))\n",
    "    ax.set_yticks(range(len(classes)))\n",
    "    ax.set_xticklabels(classes)\n",
    "    ax.set_yticklabels(classes)\n",
    "\n",
    "    # Ajout d'une barre de couleur\n",
    "    cbar = ax.figure.colorbar(heatmap, ax=ax)\n",
    "\n",
    "    # Ajout des titres\n",
    "    ax.set_xlabel('Prédictions')\n",
    "    ax.set_ylabel('Vraies étiquettes')\n",
    "    ax.set_title('Matrice de confusion')\n",
    "\n",
    "    # Affichage de la figure\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des hyperparamètres à essayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'C': [1e-2, 1e-1, 1, 1e1],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmc = SVC()\n",
    "grille = GridSearchCV(estimator=svmc, param_grid=parameters, scoring='balanced_accuracy', cv=5, verbose=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécuter la recherche de grille pour trouver la meilleure configuration de modèle en ajustant les modèles sur les données d'apprentissage et en évaluant leur performance à l'aide de la validation croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats = grille.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage du meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Le meilleur modèle :', resultats.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats.cv_results_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = resultats.best_estimator_\n",
    "y_true = y_test\n",
    "y_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On regarde si on aurait obtenu de meilleurs résultats/un autre meilleur kernel avec d'autres fonctions de scoring pour le GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_scoring_value(scoring):\n",
    "    print('>>> Scoring => ', scoring)\n",
    "    # determination du meilleur kernel pour la fonction de scoring\n",
    "    svmc = SVC()\n",
    "    grille = GridSearchCV(estimator=svmc, param_grid=parameters, scoring=scoring, cv=2)\n",
    "    resultats = grille.fit(X_train, y_train)\n",
    "    print('Le meilleur modèle :', resultats.best_params_)\n",
    "    \n",
    "    # confusion matrix & accuracy\n",
    "    svm = resultats.best_estimator_\n",
    "    y_true = y_test\n",
    "    y_pred = svm.predict(X_test)\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(accuracy_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorings = ['accuracy', 'balanced_accuracy', 'top_k_accuracy', 'average_precision', 'neg_brier_score', 'f1', 'neg_log_loss', 'precision', 'recall', 'jaccard', 'roc_auc']\n",
    "for s in scorings:\n",
    "    test_scoring_value(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recherche des meilleurs attributs au vu de leur impact sur l'erreur"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération des hyperparamètres optimaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_opti = resultats.best_params_['C']\n",
    "kernel_opti = resultats.best_params_['kernel']\n",
    "gamma_opti = resultats.best_params_['gamma']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppression un par un des attributs jusqu'à avoir l'erreur minimale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributs_removed = []\n",
    "err_fix=1\n",
    "verif=True\n",
    "users_best=users\n",
    "\n",
    "verif_count = 0\n",
    "\n",
    "while (verif):\n",
    "    print('verif n°', verif_count, attributs)\n",
    "    verif_count += 1\n",
    "    impact=[]\n",
    "    for att in attributs :\n",
    "        print('Trying to remove', att)\n",
    "        users_red=users_best.drop(columns=[att])\n",
    "        \n",
    "        Y_clean=users_red.label\n",
    "        X_clean=users_red.drop(columns=[\"label\"])\n",
    "\n",
    "        X_tmp, X_test, y_tmp, y_test = train_test_split(X_clean, Y_clean, test_size=0.3)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.5)\n",
    "        \n",
    "        svm=SVC(C=C_opti,kernel=kernel_opti, gamma=gamma_opti)\n",
    "        svm.fit(X_train, y_train)\n",
    "        y_pred=svm.predict(X_test)\n",
    "\n",
    "        erreur=1-accuracy_score(y_test,y_pred)\n",
    "\n",
    "        impact.append({\"attribut\" :att,\"erreur\":erreur})\n",
    "\n",
    "    #On retire l'attribut qui contribue le plus à  l'erreur \n",
    "    impact.sort(key=lambda x: x['erreur'])\n",
    "    err_cal=impact[0]['erreur']\n",
    "    if(err_cal>err_fix):\n",
    "        verif=False\n",
    "    else:\n",
    "        attribut_neg=impact[0]['attribut']\n",
    "        users_best=users.drop(columns=[attribut_neg])\n",
    "        attributs.remove(attribut_neg)\n",
    "        attributs_removed.append(attribut_neg)\n",
    "        print(impact[0])\n",
    "        err_fix=err_cal\n",
    "    if(len(attributs)==2):\n",
    "        verif=False\n",
    "print('END')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage des attributs qui doivent être gardés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage des attributs supprimés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributs_removed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On regarde si la matrice de confusion a changée avec les attributs en moins. Si cela a augmenté ou non la précision ou a eu une influence sur le classification_report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train.drop(columns=attributs_removed)\n",
    "#X_test = X_test.drop(columns=attributs_removed)\n",
    "\n",
    "svm=SVC(C=C_opti,kernel=kernel_opti, gamma=gamma_opti)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred=svm.predict(X_test)\n",
    "y_true = y_test\n",
    "\n",
    "display_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=users.label\n",
    "X=users.drop(columns=[\"label\",\"avg_tweet_levenshtein_similarity\"])\n",
    "attributs=[att for att in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig = pd.DataFrame(\n",
    "    {\n",
    "        \"Dimension\" : [\"Dim\" + str(x + 1) for x in range(12)], \n",
    "        \"Variance expliquée\" : pca.explained_variance_,\n",
    "        \"% variance expliquée\" : np.round(pca.explained_variance_ratio_ * 100),\n",
    "        \"% cum. var. expliquée\" : np.round(np.cumsum(pca.explained_variance_ratio_) * 100)\n",
    "    }\n",
    ")\n",
    "eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig.plot.bar(x = \"Dimension\", y = \"% variance expliquée\") # permet un diagramme en barres\n",
    "plt.axhline(y = 6.25, linewidth = .5, color = \"dimgray\", linestyle = \"--\") # ligne 17 = 100 / 6 (nb dimensions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2  # Remplacez par le nombre de composantes principales souhaitées\n",
    "X_pca = pca.transform(X)[:, :n_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_pca)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
